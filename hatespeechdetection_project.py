# -*- coding: utf-8 -*-
"""HateSpeechDetection_Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v0HvEe9xe1OylKrKH3lcQecrlL5mEvPl

# Defining the Question

### a) Specifying the Question

> Create a cross-lingual machine learning model focused on Political Hate Speech in Kenya which classifies whether an online post is deemed hate speech and the severity(sub-class) of it if so or not not hate speech (normal speech).

### b) Defining the Metric for Success

> An overall classification accuracy of 80% and Hate Class Recall of 70%.

### c) Understanding the context

> In an increasingly digital era where online social interactions are considered part of the social context, it is proving inevitable that machine learning should be used to protect people from harmful content. This has been evidenced by the multitude of instances where hate speech propagated online (mostly based on misinformation) has led to physical injury and loss of lives across the world. Government institutions should now consider online interactions as spaces where potential crimes may occur just like in the physical world.


> The goal of identifying hate speech efficiently and accurately irrespective of language is becoming a necessity. Countries like Kenya amongst other African nations have experienced the consequences of not dealing with hate speech as evidenced in previous years. Agencies such as the National Cohesion & Integration Commission were formed to help with this. Section 13 of National Cohesion and Integration Act(2008) outlines what is considered hate speech. In combination with the act an automated way of flagging hate speech would prove helpful for the institution given the country’s context which may not be similar to other countries meaning posts may not be picked/flagged by social media companies such as Twitter and Facebook as a result.


> Political hate speech is the greatest area of concern in regards to Kenya and thus we’ll be our area of focus. Looking at whether a post is Hate Speech or Normal Speech and it's severity (sub-class).

### d) Recording the Experimental Design

> The following design was used:


* Data importation
* Data Reading & Pre-processing
* EDA
* Unsupervised Topic Modeling
* Semi-Supervised Hate Speech Detection
  - Optimization/Tuning

### e) Data Relevance

> This was evaluated against the metric of success (after implementation of solution)

## Importing the libraries
"""

#@title lib installation
#installing transformers
!pip install transformers==3.1.0

#installing happytransformer
!pip install happytransformer

#tweet-preprocessor installation
!pip install tweet-preprocessor

#ekphrasis installation
!pip install ekphrasis

#emot installation
!pip install emot

#demoji installation
!pip install demoji

#Import libs
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import csv
import re
import preprocessor as tweet_proc
import demoji
from transformers import pipeline
from happytransformer import HappyTextClassification
from ekphrasis.classes.segmenter import Segmenter
from emot.emo_unicode import UNICODE_EMO, EMOTICONS
from sklearn.model_selection import train_test_split

"""## Importing the Dataset

"""

#load dataset
tweets_df = pd.read_csv('/content/sample_dataset_eagles.csv')

#check head
tweets_df.head()

#check tail
tweets_df.tail()

#check number of records
tweets_df.shape

#checking column types
tweets_df.info()

#checking summary stats
tweets_df.describe()

#checking for nulls
tweets_df.isnull().sum()

#drop nulls
tweets_df = tweets_df.dropna()

#checking for nulls
tweets_df.isnull().sum()

#check number of records
tweets_df.shape

#check duplicates
tweets_df.duplicated().sum()

#drop duplicates
tweets_df = tweets_df.drop_duplicates()

#check number of records
tweets_df.shape

"""## Pre-processing

> In this section we'll find instances of usersnames, numbers, hashtags, URLs and common emoticons and replace them with the tokens: "user","number","hashtag","url","emoticon".

> We also shorten elongated words into standard format e.g yeeeessss to yes. The hashtags that include some tokens without spacing between them, we replace them by their textual
counterparts e.g. converting hashtag “#notsexist” to “not sexist”. 

> Punctuation marks, extra delimiting characters are removed, but stop words are kept because our proposed model trains the sequence of words in a text directly. We also convert all tweets to lower case.
"""

#change column name
tweets_df.rename(columns={'hate_speech(1=hspeech, 0=nohspeech)': "hate_speech"}, inplace=True)

# #create copy to work with
# tweet_df = tweets_df.copy(deep=True)

# #drop default hashtag column and derive from tweet
# tweet_df = tweet_df.drop("hashtags", axis=1)

# #extract tags from tweets
# tweet_df['hashtag'] = tweet_df['tweet'].apply(lambda x: re.findall(r"#(\w+)", x))

# #using tweet preprocessor to clean tweets
# #set options
# tweet_proc.set_options(tweet_proc.OPT.URL, tweet_proc.OPT.EMOJI,tweet_proc.OPT.MENTION,tweet_proc.OPT.HASHTAG,tweet_proc.OPT.RESERVED,
#                        tweet_proc.OPT.SMILEY)

#forming a separate feature for cleaned tweets
#create empty col
#tweet_df['text'] = ''
#tweet_df['text'] = str(tweet_df['tweet'])
# for i,v in enumerate(tweet_df['tweet']):
#   tweet_df.loc[i,"text"] = tweet_proc.clean(v)

#segment any hashtags
#seg_tw = Segmenter(corpus="twitter")
#tweet_df['hashtag'] =str(tweet_df['hashtag'])
# a = []
# for i in range(0,len(tweet_df)):
#  if tweet_df['hashtag'][i] != a:
#   listToStr1 = ' '.join([str(elem) for elem in tweet_df['hashtag'][i]])
#   tweet_df.loc[i,'Segmented#'] = seg_tw.segment(listToStr1)

#change to lower case all the  tweets and remove numbers
# def preprocess_data(data):
#   #Removes Numbers
#   #data = data.astype(str).str.replace('\d+', '')
#   lower_text = data.str.lower()

# prep_tweets = preprocess_data(tweet_df['text'])
# tweet_df['text'] = prep_tweets


# #remove punctuation
# def remove_punctuation(words):
#   new_words = []
#   for word in words:
#     new_word = re.sub(r'[^\w\s]', '', (word))
#     if new_word != '':
#       new_words.append(new_word)
#   return new_words

#   words = lower_text.apply(remove_punctuation)
#   return pd.DataFrame(words)

# prep_tweets = remove_punctuation(tweet_df['text'])
# tweet_df['text'] = prep_tweets


#view cleaned df
#tweet_df.head()

#mentioned tokens: <user>,<number>,<hashtag>,<url>,<emoticon>
#Pre-process tweets
#using tweet-preprocessor3 lib for tweet tokenization

# to leverage word statistics from Twitter
#seg_tw = Segmenter(corpus = "twitter")
#cleaning the following:
# URL	-> .OPT.URL
# Mention	-> .OPT.MENTION
# Hashtag	-> .OPT.HASHTAG
# Reserved Words	-> .OPT.RESERVED
# Emoji	-> .OPT.EMOJI
# Smiley	-> .OPT.SMILEY


#set options
tweet_proc.set_options(tweet_proc.OPT.URL, tweet_proc.OPT.EMOJI,tweet_proc.OPT.MENTION,tweet_proc.OPT.HASHTAG,tweet_proc.OPT.RESERVED,
                       tweet_proc.OPT.SMILEY)


#create a txt file of the tweet list to use in the clean tweets package

#define a unique delimiter for each record(tweet)
DELIMITER = "|"

#make a list of the tweet col
tweet = list(tweets_df['tweet'])

#write to txt file named tweet
with open('tweet.txt', 'w') as outfile:
    writer = csv.writer(outfile, delimiter=DELIMITER)
    writer.writerow(tweet)

#read the txt file to see row
with open('tweet.txt', 'r') as infile:
    reader = csv.reader(infile, delimiter=DELIMITER)
    for row in reader:
        print(row)

#use the method clean_file to clean the tweets and save
tweet_proc.clean_file("/content/tweet.txt")

#import the cleaned txt file as a df
df = pd.read_csv("/content/4eAXsoe9JIsp_tweet.txt",delimiter=DELIMITER , header=None)

#transpose to have cols and rows correct
df = df.transpose()

#check shape to confirm
df.shape

#rename col of tweet df
df.rename(columns={0: "cleaned_tweet"}, inplace=True)

# #create copy to work with
tweet_df = tweets_df.copy(deep=True)

#reset index
tweet_df.reset_index(inplace=True)

#append it as a column back to the original dataframe
tweet_df['cleaned_tweet']= df

#punctuation
tweet_df['cleaned_tweet'] = tweet_df['cleaned_tweet'].str.replace('[^\w\s]', '').astype(str) 

#remove digits
twl_1 = list(tweet_df['cleaned_tweet'])
no_num = []
for w in range(len(twl_1)):
  remove_num = re.sub(r'\d+', '', twl_1[w])
  no_num.append(remove_num)

#create a dataframe to hold the new tweets
no_num_df = pd.DataFrame(no_num, columns=['no_num_twt'])

#append it as a column back to the original dataframe
tweet_df['no_num_twt']= no_num_df

#lower case
tweet_df['no_num_twt'] = tweet_df['no_num_twt'].apply(lambda x: x.lower())

#drop old tweet col before cleaning
tweet_df = tweet_df.drop(['tweet','cleaned_tweet','index'], axis=1)

#rename col of tweet df
tweet_df.rename(columns={'no_num_twt': "tweet"}, inplace=True)

#preview new df
tweet_df.tail()



"""## Feature Engineering"""

# use tweet-preprocessor to include some useful information
# fields which can act as features for our classifiers
# Include the hashtag text after segmenting into meaningful tokens using the ekphrasis segmenter for the twitter corpus

# save information such as URLs, name mentions, quantitative values and smileys
# extract emojis and processed using emoji2vec to obtain a semantic vector representing the particular emoji

"""# Exploratory Data Analysis"""

#proportion of hate speech

# let us plot histograms to visualize patterns in the data
tweet_df.hist(figsize = (20,10))
plt.show()

# Boxplots to Visualize outliers of our numerical columns 
plt.figure(figsize = (20,10))
ax = sns.boxplot(data=tweet_df, orient="v", palette="Set2")
plt.title('Checking for outliers using boxplots')
# The boxplots below indicate the outliers in each of the numerical columns

# let us see how the labels are distributed in our dataset
# plt_hate = tweet_df.copy(deep=True)
# plt_hate.rename
# tweets_df.rename(columns={'hate_speech(1=hspeech, 0=nohspeech)': "hate_speech"}, inplace=True)


#plot countplot
plt.figure(figsize=(10,5))
sns.countplot(x="hate_speech", data = tweet_df)
plt.title("Number of Hate Speech Posts")

plt.show()


# neutral labels are the highest in our data



# # extracting the number of examples of each class
# hate = tweet_df[tweet_df['hate_speech'] == 1]
# not_hate = tweet_df[tweet_df['hate_speech'] == 0]

# # bar plot of the 3 classes
# #plt.rcParams['figure.figsize'] = (7, 5)
# plt.bar(hate, label="Hate Speech", color='red')
# plt.bar(not_hate, label="Not Hate Speech", color='blue')
# plt.legend()
# plt.ylabel('Number of examples')
# plt.title('Propertion of examples')
# plt.show()

# Most common words
from wordcloud import WordCloud
from nltk import FreqDist
#Frequency of words
fdist = FreqDist(tweet_df['hashtags'])
#WordCloud
wc = WordCloud(width=800, height=400, max_words=50).generate_from_frequencies(fdist)
plt.figure(figsize=(12,10))
plt.imshow(wc, interpolation="bilinear")
plt.axis("off")
plt.show()

"""# Implementing the Solution

## Unsupervised Topic Modeling

### Build pipeline for text classification
"""

#Instantiate class pipeline from transformers
#classifier = pipeline("zero-shot-classification")
# classifier = pipeline("zero-shot-classification", device=0) # to utilize GPU

# Zero-shot classification in 100 languages
# A pipeline for languages other than English,
# a trained cross-lingual model on top of XLM RoBERTa:

classifier = pipeline("zero-shot-classification", model='joeddav/xlm-roberta-large-xnli')

#multi-lingual
sequence = "Wewe ni mavi ya kuku"
candidate_labels = ["violent", "offensive", "profane"]

res = classifier(sequence, candidate_labels,multi_label=True)

print(res['labels'])
print(res['scores'])

# for multi-class classification, we pass multi_class=True. 
# In this case, the scores will be independent, but each will fall between 0 and 1.

#e.g.
#sequence = "Who are you voting for in 2020?"

#candidate_labels = ["politics", "public health", "economics", "elections"]

#classifier(sequence, candidate_labels, multi_class=True)


#candidate_labels = ["politics", "public health", "economics", "elections"]

candidate_labels = ["violent", "offensive", "profane"]


# #cluster tweets to sub-classes
# twl_1 = list(tweet_df['tweet'])
# sub_group = []
# for t in range(len(twl_1)):
#   preds = classifier(twl_1, candidate_labels, multi_label=True)
#   sub_group.append(preds)

#create a dataframe to hold the new tweets
#no_num_df = pd.DataFrame(no_num, columns=['no_num_twt'])

#from transformers import pipeline english only
#classifier = pipeline("zero-shot-classification")

sequence = "Wewe ni mavi ya kuku"
candidate_labels = ["violent", "offensive", "profane"]

classifier(sequence, candidate_labels,multi_label=True)



"""## Semi-Supervised Hate Speech Detection

### Optimization/Tuning
"""

#Create A HappyTextClassification Object (ROBERTA or BERT)
#happy_tc = HappyTextClassification("ROBERTA", "roberta-base")

#Create A HappyTextClassification Object (ROBERTA or BERT)
#Bert used with 2 labels (Hate/Non_hate)
happy_tc = HappyTextClassification("BERT", "Hate-speech-CNERG/dehatebert-mono-english", 2)

#Hate-speech-CNERG/bert-base-uncased-hatexplain
#Create A HappyTextClassification Object (ROBERTA or BERT)
#happy_tc = HappyTextClassification("BERT", "Hate-speech-CNERG/bert-base-uncased-hatexplain")

#predict hate speech or not
result = happy_tc.classify_text("xx xxx")
print(result)
print(result.label)
print(result.score)

"""### Train & Eval"""

#create a train_eval file from the dataset
#rename cols of tweet df
tweet_df.rename(columns={"hate_speech": "label","tweet": "text" }, inplace=True)

#convert label column to int
tweet_df.label = tweet_df.label.astype(int)

# split the data into train and test set
train_eval, test = train_test_split(tweet_df, test_size=0.2, random_state=101, shuffle=False)

#extract text and label
train_eval = train_eval[['text', 'label']]
test = test[['text', 'label']]

#save dfs to csv
train_eval.to_csv("train-eval.csv")
test.to_csv("test.csv")

#fine tune to the dataset
#happy_tc.train("../../data/tc/train-eval.csv")

#fine tune to the dataset
happy_tc.train("/content/train-eval.csv")

#eval the fine tuned model
result = happy_tc.eval("/content/train-eval.csv")
print(type(result))  # <class 'happytransformer.happy_trainer.EvalResult'>
print(result)  # EvalResult(eval_loss=0.007262040860950947)
print(result.loss)  # 0.007262040860950947



#test the model
result = happy_tc.test("/content/test.csv")
print(type(result))  # <class 'list'>
print(result)  # [TextClassificationResult(label='LABEL_1', score=0.9998401999473572), TextClassificationResult(label='LABEL_0', score=0.9772131443023682)...
print(type(result[0]))  # <class 'happytransformer.happy_text_classification.TextClassificationResult'>
print(result[0])  # TextClassificationResult(label='LABEL_1', score=0.9998401999473572)
print(result[0].label)  # LABEL_1

"""# Challenging the Solution

# Follow up questions

## a) Did we have the right data?

## b) Do we need other data to answer our question?

## c) Did we have the right question?
"""